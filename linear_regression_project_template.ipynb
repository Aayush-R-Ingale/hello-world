{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aayush-R-Ingale/hello-world/blob/main/linear_regression_project_template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQv-p8G0urN-"
      },
      "source": [
        "# Linear Regression Project: Student Performance Prediction\n",
        "\n",
        "## Objective\n",
        "Build a linear regression model to predict student performance based on various factors including study habits, previous scores, and lifestyle factors.\n",
        "\n",
        "## Instructions\n",
        "Fill in the code cells below to complete the linear regression analysis. Follow the comments carefully and implement the required functionality in each cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jooNvsMfoZkW"
      },
      "outputs": [],
      "source": [
        "# Import Required Libraries\n",
        "\n",
        "# This cell imports all necessary libraries for data manipulation, visualization, and machine learning\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import kagglehub, os, glob\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Libraries specific to data preprocessing and the model itself\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "\n",
        "# Set visualization style\n",
        "sns.set_style(\"whitegrid\")\n",
        "sns.set_palette(\"RdBu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Dataset\n",
        "\n",
        "# This cell downloads and loads the student performance dataset. This step has been done for you!\n",
        "\n",
        "# Download dataset from Kaggle\n",
        "path = kagglehub.dataset_download(\"nikhil7280/student-performance-multiple-linear-regression\")\n",
        "print(\"Downloaded to:\", path)\n",
        "print(\"Files:\", os.listdir(path))\n",
        "\n",
        "# Find the CSV file in the downloaded folder\n",
        "csv_candidates = glob.glob(os.path.join(path, \"*.csv\"))\n",
        "assert len(csv_candidates) >= 1, \"No CSV found in the downloaded dataset folder.\"\n",
        "csv_path = csv_candidates[0]\n",
        "print(\"Using CSV:\", csv_path)\n",
        "\n",
        "# Load the data into a pandas DataFrame. For more information on what pandas is, and why is it so important\n",
        "# in ML, please follow this link: https://medium.com/@mayurkoshti12/why-is-pandas-popular-in-data-science-5a14ea7e25b4\n",
        "data = pd.read_csv(csv_path)"
      ],
      "metadata": {
        "id": "eN6fd4L1ow3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initial Data Exploration\n",
        "\n",
        "# Whenever we work with ML, it is crucial to explore the data to understand its structure and to know the values we have in the dataset itself.\n",
        "# Without properly knowing what data you are working with, it is very easy to feed the model corrupted or null data which can slow down the overall\n",
        "# training process.\n",
        "\n",
        "# By convention, we call each column a feature of the dataset, with the target variable being the target. In this dataset, you will notice we have 6\n",
        "# columns, which means we have 5 features and 1 target.\n",
        "\n",
        "# TODO: Display the first 5 rows of the dataset to understand its structure.\n"
      ],
      "metadata": {
        "id": "E7xDQVEZo6r7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Information\n",
        "\n",
        "# TODO: Get basic information about the dataset including data types and null values (Hint: Use the .info() method on the DataFrame)\n"
      ],
      "metadata": {
        "id": "o2gzSv30pF9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Shape\n",
        "\n",
        "# The shape of the dataset is another critical aspect of machine learning. Without knowing the shape, you will essentially have no idea the amount\n",
        "# of rows and columns you are working with. The shape of a dataset usually is structured as a tuple: (x, y) where x is the number of rows in the\n",
        "# dataset and y is the number of columns.\n",
        "\n",
        "# TODO: Print the shape of the dataset.\n"
      ],
      "metadata": {
        "id": "Zql3JgfJyOO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check null values\n",
        "\n",
        "# Null values are a problem in ML datasets where a certain feature does not have a value associated with it. This can cause the model\n",
        "# training to take longer to find the correlation between a feature and the target variable. For example, if we have 4 data values to\n",
        "# find the correlation between square footage and housing prices and 2 of the square footage values are empty, it will take longer for\n",
        "# the model to find the correlation between the square footage and housing prices.\n",
        "\n",
        "# TODO: Find the percentage of null values of each feature, by dividing the sum of total null values by the number of rows in the dataset\n",
        "# (HINT: you only need to divide by the first index of the shape, so shape[0])\n"
      ],
      "metadata": {
        "id": "zkoA5xhqxEdA"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Analysis\n",
        "\n",
        "# Here, we've created a correlation heatmap to visualize relationships between numerical variables. A correlation matrix tells you the relationship of one\n",
        "# feature to another. This is another important step to visualize how the data's features relate to each other!\n",
        "\n",
        "plt.figure(figsize = (10,6))\n",
        "sns.heatmap(data.select_dtypes(exclude = object).corr(), annot = True, fmt = \".2f\", linewidths = 0.2)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "LX8qU9shpLmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preprocessing\n",
        "\n",
        "# In ML, the model cannot understand text, like we can. For one of the features, you will notice that there is a non-numerical value. To deal with this,\n",
        "# we will need to use a LabelEncoder. This tool converts text into numerical values that the model can understand and work with! We'll leave it to you to\n",
        "# figure out which features need to be encoded.\n",
        "\n",
        "# TODO: Encode categorical variables using LabelEncoder\n",
        "\n"
      ],
      "metadata": {
        "id": "NcVFafTJpeRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify Encoding\n",
        "\n",
        "# Now that you've encoded the relevant features, display the first few rows again just to confirm that the encoding worked.\n",
        "\n",
        "# TODO: Display the first 5 rows again to confirm encoding\n"
      ],
      "metadata": {
        "id": "-Hhut6aMp9j8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare Features and Target Variables\n",
        "\n",
        "# Now comes the fun part! Now that you've successfully examined and cleaned the data, you have completed the data preprocessing step. Doing that was\n",
        "# critical because without proper structured and clean data, the model's performance will take a drastic hit or not even fit as intended.\n",
        "\n",
        "# That being said, it is finally time to begin the process of training. So first, we need to split the data into the target and training sets.\n",
        "# You will have noticed that the dataset is 6 columns as of now, but we need to have all the values for training (our 5 features) and target seperate\n",
        "# in order to train our model.\n",
        "\n",
        "# TODO: Separate the dataset into features (X) and target variable (y)\n",
        "# 1. Create 'Train' by dropping the 'Performance Index' column (this will be our features)\n",
        "# 2. Create 'Target' by selecting only the 'Performance Index' column (this is what we want to predict)\n",
        "\n",
        "Train =\n",
        "Target ="
      ],
      "metadata": {
        "id": "sN4TDHTaqDdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify Feature Preparation\n",
        "\n",
        "# Now that we have split our dataset into Test and Train, we need to verify that the they are what we are looking for. You can print out a sample\n",
        "# of the Train set to verify that it is correct.\n",
        "\n",
        "# TODO: Display a sample of the feature data to verify it looks correct\n"
      ],
      "metadata": {
        "id": "viZYld1eqKeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify Target Variable\n",
        "\n",
        "# Do the same for the target set!\n",
        "\n",
        "# TODO: Display the first 5 values of the target variable\n"
      ],
      "metadata": {
        "id": "aCrLy-ZfqLc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train-Test Split\n",
        "\n",
        "# Here comes the final part, and probably the most recognized! We need to split the data into a test and train dataset.\n",
        "# We split our data because we want to evaulate the performace of our model on the test set while training the model on the train set.\n",
        "# This way we can test our model on \"fresh\" data that it won't have been trained on.\n",
        "\n",
        "# TODO: Split the data into training and testing sets\n",
        "# 1. Use train_test_split with Train and Target as inputs\n",
        "# 2. Set test_size=0.2 (20% for testing, 80% for training)\n",
        "# 3. Set random_state=42 for reproducible results. This essentially means that if you re-run the code, you will get the same data points used!\n",
        "# 4. Assign to X_train, X_test, y_train, y_test\n",
        "\n",
        "# Your code here:\n",
        "X_train, X_test, y_train, y_test =\n",
        "\n",
        "# Print the shapes to verify the split\n",
        "print(\"X_train shape: \", X_train.shape)\n",
        "print(\"y_train shape: \", y_train.shape)\n",
        "print(\"X_test shape: \", X_test.shape)\n",
        "print(\"y_test shape: \", y_test.shape)"
      ],
      "metadata": {
        "id": "hbVEA-LSqM_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 12: Create and Train the Model\n",
        "\n",
        "# Finally, create the Linear Regression model and fir it on the newly created sets above! We will leave it to you to figure out which sets to use!\n",
        "\n",
        "# TODO: Create a Linear Regression model and train it\n",
        "# 1. Create a LinearRegression object and assign it to 'model'\n",
        "# 2. Fit the model using the training data (X_train and y_train)\n",
        "\n",
        "model =\n"
      ],
      "metadata": {
        "id": "hG6p08dKqZA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate Training Performance\n",
        "\n",
        "# Now that we have fitted and trained the model, we need to evaluate how accurate it was on the training set. The higher the score here, the better\n",
        "# and more accurate our model is!\n",
        "\n",
        "# TODO: Calculate the R² score on the training data\n",
        "\n",
        "training_score =\n",
        "print(f\"Training R² Score: {training_score:.4f}\")"
      ],
      "metadata": {
        "id": "FvnQPfkMqfae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make Predictions\n",
        "\n",
        "# Now, we can use the model that we fitted to make predictions on the test set! We can use this to compare with the actual values.\n",
        "\n",
        "# TODO: Use the trained model to make predictions on the test set\n",
        "# 1. Use model.predict() on X_test to get predictions\n",
        "# 2. Round the predictions to 1 decimal place using np.round() with decimals=1\n",
        "# 3. Create a DataFrame showing actual vs predicted values\n",
        "\n",
        "predict =\n",
        "\n",
        "# Create a comparison DataFrame\n",
        "comparison_df = pd.DataFrame({\n",
        "    \"Actual Performance\": y_test,\n",
        "    \"Predicted Performance\": predict\n",
        "})\n",
        "\n",
        "# Display the comparison\n",
        "print(comparison_df)"
      ],
      "metadata": {
        "id": "vBsl67yQqhU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a Single Prediction\n",
        "\n",
        "# Now, it's your turn! Fill out the example student data to see how well a student may do based on the stats you gave them on their exams.\n",
        "\n",
        "# TODO: Test the model with a single example\n",
        "# Use the provided values to make a prediction for one student\n",
        "\n",
        "# Example student data\n",
        "hours_studied = 7\n",
        "previous_score = 99\n",
        "extra_curricular = 1  # 1 for Yes, 0 for No\n",
        "sleep_hours = 9\n",
        "practice_exams_studied = 1\n",
        "\n",
        "single_prediction = [[[hours_studied, previous_score, extra_curricular, sleep_hours, practice_exams_studied]]]\n",
        "\n",
        "print(f\"Predicted Performance for sample student: {single_prediction[0]:.2f}\")"
      ],
      "metadata": {
        "id": "JsF23VoKqwwM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}